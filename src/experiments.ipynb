{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch\n",
    "# from config import *\n",
    "# from torch import nn\n",
    "# from dataset import CoLADataset\n",
    "\n",
    "# import wandb\n",
    "# from tqdm import tqdm, trange\n",
    "# from model import BertClassifier\n",
    "# from transformers import BertTokenizer, AdamW , get_linear_schedule_with_warmup\n",
    "# from config import *\n",
    "\n",
    "# class CoLADataset(Dataset):\n",
    "#     def __init__(self, path, tokenizer):\n",
    "#         self.df = pd.read_csv(path, \n",
    "#                                 sep='\\t', \n",
    "#                                 names = [\"x\", \"label\", \"y\", \"sentence\"], \n",
    "#                                 header= None\n",
    "#         )[['label', 'sentence']]\n",
    "        \n",
    "#         self.data = self.df.to_dict(orient='records')\n",
    "#         self.tokenizer = tokenizer\n",
    "#         print(f\"Loaded Dataset: {len(self.data)}\")\n",
    "\n",
    "#     def get_dataloader(self):\n",
    "#         return DataLoader(self.data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         print(\"Passed Index: \", idx)\n",
    "#         data_pt = self.data[idx]\n",
    "\n",
    "#         tokenized_sentence = self.tokenizer.encode_plus(\n",
    "#             data_pt['sentence'], \n",
    "#             return_tensors= 'pt',\n",
    "#             padding=\"max_length\",\n",
    "#             max_length = 15,\n",
    "#             truncation=True\n",
    "#         )\n",
    "#         label = torch.tensor([data_pt[\"label\"]])\n",
    "\n",
    "#         return {\n",
    "#             \"labels\": label, \n",
    "#             **tokenized_sentence\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# dataset = CoLADataset('data/in_domain_dev.tsv', tokenizer)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('data/in_domain_dev.tsv', \n",
    "#             sep='\\t', \n",
    "#             names = [\"x\", \"label\", \"y\", \"sentence\"], \n",
    "#             header= None\n",
    "#         )[['label', 'sentence']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# for batch in trainloader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# del trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# trainloader = DataLoader(trainer.train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# batch = next(iter(trainloader))\n",
    "# # l, i, t, a = b\n",
    "# batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.bert import BertModel\n",
    "\n",
    "# class BertClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels):\n",
    "#         super().__init__()\n",
    "#         self.bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "#         self.classifier = nn.Linear(768, num_labels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         cls_token = self.bert_model(x).last_hidden_state[:, 0, :]\n",
    "#         return self.classifier(cls_token)\n",
    "    \n",
    "# model = BertClassifier(num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bert_model(inputs['input_ids'].squeeze()).last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "# model(inputs['input_ids'].squeeze()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # inputs, labels = (batch['input_ids'], batch['token_type_ids'], batch['attention_mask']), batch['labels']\n",
    "\n",
    "# # inputs = {\n",
    "# #     'input_ids': batch['input_ids'], \n",
    "# #     'token_type_ids': batch['token_type_ids'], \n",
    "# #     'attention_mask': batch['attention_mask']\n",
    "# # }\n",
    "\n",
    "# inputs, labels = batch\n",
    "\n",
    "# # outputs = trainer.model(inputs['input_ids'])\n",
    "# # outputs\n",
    "# inputs['input_ids'].squeeze()\n",
    "# # inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwingman-shishir\u001b[0m (\u001b[33mwingman-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/shishirjoshi/.netrc\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.2.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.10.output.adapter.proj_down.bias', 'bert.encoder.layer.5.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.2.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.9.output.adapter.proj_up.weight', 'bert.encoder.layer.2.output.adapter.proj_down.weight', 'bert.encoder.layer.3.output.adapter.proj_up.bias', 'bert.encoder.layer.7.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.4.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.3.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.1.output.adapter.proj_down.weight', 'bert.encoder.layer.0.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.1.output.adapter.proj_up.bias', 'bert.encoder.layer.2.output.adapter.proj_down.bias', 'bert.encoder.layer.4.output.adapter.proj_up.bias', 'bert.encoder.layer.0.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.4.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.8.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.6.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.9.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.8.output.adapter.proj_down.weight', 'bert.encoder.layer.3.output.adapter.proj_down.bias', 'bert.encoder.layer.5.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.1.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.6.output.adapter.proj_down.weight', 'bert.encoder.layer.11.output.adapter.proj_up.weight', 'bert.encoder.layer.0.output.adapter.proj_down.weight', 'bert.encoder.layer.5.output.adapter.proj_down.weight', 'bert.encoder.layer.10.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.8.output.adapter.proj_up.bias', 'bert.encoder.layer.9.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.9.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.6.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.7.output.adapter.proj_down.bias', 'bert.encoder.layer.2.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.0.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.3.output.adapter.proj_down.weight', 'bert.encoder.layer.3.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.11.output.adapter.proj_down.bias', 'bert.encoder.layer.11.output.adapter.proj_down.weight', 'bert.encoder.layer.7.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.11.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.8.output.adapter.proj_down.bias', 'bert.encoder.layer.4.output.adapter.proj_down.weight', 'bert.encoder.layer.6.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.8.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.1.output.adapter.proj_down.bias', 'bert.encoder.layer.10.output.adapter.proj_up.bias', 'bert.encoder.layer.5.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.5.output.adapter.proj_up.bias', 'bert.encoder.layer.7.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.9.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.4.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.3.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.0.output.adapter.proj_up.weight', 'bert.encoder.layer.11.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.8.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.4.output.adapter.proj_up.weight', 'bert.encoder.layer.8.output.adapter.proj_up.weight', 'bert.encoder.layer.7.output.adapter.proj_up.bias', 'bert.encoder.layer.10.output.adapter.proj_up.weight', 'bert.encoder.layer.9.output.adapter.proj_down.weight', 'bert.encoder.layer.6.output.adapter.proj_up.bias', 'bert.encoder.layer.10.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.5.output.adapter.proj_up.weight', 'bert.encoder.layer.7.output.adapter.proj_down.weight', 'bert.encoder.layer.6.output.adapter.proj_up.weight', 'bert.encoder.layer.0.output.adapter.proj_up.bias', 'bert.encoder.layer.7.output.adapter.proj_up.weight', 'bert.encoder.layer.10.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.10.output.adapter.proj_down.weight', 'bert.encoder.layer.6.output.adapter.proj_down.bias', 'bert.encoder.layer.5.output.adapter.proj_down.bias', 'bert.encoder.layer.0.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.3.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.4.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.2.output.adapter.proj_up.weight', 'bert.encoder.layer.9.output.adapter.proj_up.bias', 'bert.encoder.layer.3.output.adapter.proj_up.weight', 'bert.encoder.layer.5.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.1.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.1.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.10.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.6.attention.output.adapter.proj_down.bias', 'bert.encoder.layer.2.output.adapter.proj_up.bias', 'bert.encoder.layer.1.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.4.output.adapter.proj_down.bias', 'bert.encoder.layer.11.attention.output.adapter.proj_up.bias', 'bert.encoder.layer.11.output.adapter.proj_up.bias', 'bert.encoder.layer.2.attention.output.adapter.proj_down.weight', 'bert.encoder.layer.0.output.adapter.proj_down.bias', 'bert.encoder.layer.11.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.1.output.adapter.proj_up.weight', 'bert.encoder.layer.8.attention.output.adapter.proj_up.weight', 'bert.encoder.layer.9.output.adapter.proj_down.bias', 'bert.encoder.layer.7.attention.output.adapter.proj_up.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/shishirjoshi/development/birds/PETL/Adapter-BERT/src/wandb/run-20230413_183343-fsj5fgpm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wingman-ml/shishir-c4ai-adapter-bert/runs/fsj5fgpm\" target=\"_blank\">glad-glade-15</a></strong> to <a href=\"https://wandb.ai/wingman-ml/shishir-c4ai-adapter-bert\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/wingman-ml/shishir-c4ai-adapter-bert\" target=\"_blank\">https://wandb.ai/wingman-ml/shishir-c4ai-adapter-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/wingman-ml/shishir-c4ai-adapter-bert/runs/fsj5fgpm\" target=\"_blank\">https://wandb.ai/wingman-ml/shishir-c4ai-adapter-bert/runs/fsj5fgpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shishirjoshi/.virtualenvs/llm_playground/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                                                       | 0/268 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▋                                                                                                                                                                            | 1/268 [01:27<6:29:47, 87.59s/it]\u001b[A\n",
      "  1%|█▎                                                                                                                                                                           | 2/268 [02:50<6:15:10, 84.62s/it]\u001b[A\n",
      "  1%|█▉                                                                                                                                                                           | 3/268 [04:11<6:06:10, 82.91s/it]\u001b[A\n",
      "  1%|██▌                                                                                                                                                                         | 4/268 [06:44<7:24:50, 101.10s/it]\u001b[A\n",
      "  0%|                                                                                                                                                                                         | 0/1 [06:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcompute_loss(outputs, labels)\n\u001b[1;32m     33\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m trainer\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.virtualenvs/llm_playground/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/llm_playground/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import *\n",
    "from torch import nn\n",
    "from dataset import CoLADataset\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm, trange\n",
    "from model import BertClassifier\n",
    "from transformers import BertTokenizer, AdamW , get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from train import Trainer\n",
    "\n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "# Setting Up DataLoaders and Optimizars\n",
    "trainloader = DataLoader(trainer.train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valloader = DataLoader(trainer.val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "trainer.configure_optimizers()\n",
    "\n",
    "# Training Loop Starts Here\n",
    "for e in trange(1):\n",
    "    train_loss = 0.0\n",
    "    # Training Step\n",
    "    for batch in tqdm(trainloader):\n",
    "        inputs, labels = batch\n",
    "        if trainer.gpu_present:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = trainer.model(inputs['input_ids'].squeeze())\n",
    "        loss = trainer.compute_loss(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        trainer.optimizer.step()\n",
    "        trainer.scheduler.step()\n",
    "\n",
    "    # Validation Step\n",
    "    valid_loss += 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valloader):\n",
    "            inputs, labels = batch['sentence'], batch['labels']\n",
    "            if trainer.gpu_present:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = trainer.model(**inputs)\n",
    "            loss = trainer.compute_loss(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    wandb.log({\n",
    "        'epoch': e,\n",
    "        'train_loss': train_loss/len(trainloader),\n",
    "        'val_loss': valid_loss/len(valloader)\n",
    "    })\n",
    "    print(f'Epoch {e}\\t\\tTraining Loss: {train_loss/len(trainloader)}\\t\\tValidation Loss: {valid_loss/len(valloader)}')\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
